{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP0aCt+916xFELk/O3NBBuS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Implementing Tokenization**\n","\n","Tokenizers are essential tools in natural language processing that break down text into smaller units called tokens. These tokens can be words, characters, or subwords, making complex text understandable to computers. By dividing text into manageable pieces, tokenizers enable machines to process and analyze human language, powering various language-related applications like translation, sentiment analysis, and chatbots. Essentially, tokenizers bridge the gap between human language and machine understanding."],"metadata":{"id":"bcuX6ftOdX-a"}},{"cell_type":"markdown","source":["### Install necessary libraries"],"metadata":{"id":"N4Lpj8G-dwnR"}},{"cell_type":"code","source":["pip install nltk spacy transformers gensim tensorflow keras torchtext"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aA1O2IBOibm2","executionInfo":{"status":"ok","timestamp":1737356857968,"user_tz":-300,"elapsed":3537,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"cbdd6b5a-5646-4d01-d5d9-a81cade525cf"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.17.1)\n","Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (3.5.0)\n","Collecting torchtext\n","  Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.12.23)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.69.0)\n","Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.17.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras) (0.0.8)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras) (0.13.1)\n","Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from torchtext) (2.5.1+cu121)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.4.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.6.85)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.3.0->torchtext) (1.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras) (2.18.0)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n","Downloading torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchtext\n","Successfully installed torchtext-0.18.0\n"]}]},{"cell_type":"code","source":["# Import necessary libraries\n","import nltk\n","import spacy\n","from transformers import AutoTokenizer\n","from gensim.utils import tokenize as gensim_tokenize\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","import re"],"metadata":{"id":"aC4BObyfdtqr","executionInfo":{"status":"ok","timestamp":1737356945685,"user_tz":-300,"elapsed":407,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["### 1. Tokenization with NLTK"],"metadata":{"id":"BOh0ncLFdhdN"}},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QIW8kLpZdFSn","executionInfo":{"status":"ok","timestamp":1737357337180,"user_tz":-300,"elapsed":401,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"35888e99-6ff2-4cb7-c06f-1b2265f3a436"},"outputs":[{"output_type":"stream","name":"stdout","text":["Word tokens:  ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'let', \"'s\", 'learn', 'tokenization', '.']\n","Sentence Tokens:  ['Natural Language Processing is amazing!', \"let's learn tokenization.\"]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","\n","# donload necessary recources\n","nltk.download('punkt_tab')\n","\n","text = \"Natural Language Processing is amazing! let's learn tokenization.\"\n","\n","#word tokenization\n","word_tokens=word_tokenize(text)\n","print(\"Word tokens: \",word_tokens)\n","\n","# sentense tokenization\n","sentence_token=sent_tokenize(text)\n","print(\"Sentence Tokens: \",sentence_token)"]},{"cell_type":"markdown","source":["### 2. Tokenization with SpaCy\n"],"metadata":{"id":"kE4BNEP5kUtI"}},{"cell_type":"code","source":["import spacy\n","# Load english model\n","nlp=spacy.load(\"en_core_web_sm\")\n","\n","text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","# process text\n","doc=nlp(text)\n","# word tokens\n","word_tokens=[token.text for token in doc]\n","print(\"Words token: \",word_tokens)\n","\n","# sentence tokens\n","sentence_tokens=[sent.text for sent in doc.sents]\n","print(\"Senetence tokens: \",sentence_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QV2qmwsQkVvj","executionInfo":{"status":"ok","timestamp":1737357546847,"user_tz":-300,"elapsed":925,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"36b5467c-9c06-462b-c13f-ac7c0e5e40c7"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Words token:  ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'learn', 'tokenization', '.']\n","Senetence tokens:  ['Natural Language Processing is amazing!', \"let's learn tokenization.\"]\n"]}]},{"cell_type":"markdown","source":["### 3. Tokenization with Hugging Face Transformers\n"],"metadata":{"id":"ODVLayu1lOEK"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","# load model\n","tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","# Word tokenization\n","word_tokens=tokenizer.tokenize(text)\n","print(\"Word Tokens: \",word_tokens)\n","\n","# Subword tokenization\n","words_ids=tokenizer.encode(text,add_special_tokens=True)\n","print(\"Tokens IDs : \",words_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iINd0_OIlE67","executionInfo":{"status":"ok","timestamp":1737357860274,"user_tz":-300,"elapsed":396,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"3fafd935-49f3-4213-b93c-feeb5c7fa948"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokens:  ['natural', 'language', 'processing', 'is', 'amazing', '!', 'let', \"'\", 's', 'learn', 'token', '##ization', '.']\n","Tokens IDs :  [101, 3019, 2653, 6364, 2003, 6429, 999, 2292, 1005, 1055, 4553, 19204, 3989, 1012, 102]\n"]}]},{"cell_type":"markdown","source":["### 4. Tokenization with Python's str.split() (Basic Approach)"],"metadata":{"id":"k0AVFlNOrL2G"}},{"cell_type":"code","source":["text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","word_tokens1=text.split()\n","print(\"Words Tokens: \",word_tokens1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3fDVGtfyrNCl","executionInfo":{"status":"ok","timestamp":1737359207478,"user_tz":-300,"elapsed":8,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"f0c707df-612d-4c18-c9ca-daeb0ad96932"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Words Tokens:  ['Natural', 'Language', 'Processing', 'is', 'amazing!', \"Let's\", 'learn', 'tokenization.']\n"]}]},{"cell_type":"markdown","source":["### 5. Tokenization with re (Regular Expressions)\n"],"metadata":{"id":"gTI27FOhrd5E"}},{"cell_type":"code","source":["import re\n","\n","text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","# custom regex\n","word_tokens = re.findall(r'\\b\\w+\\b', text)\n","print(\"Word Tokens: \",word_tokens)\n","\n","# Regex for sentence tokenization\n","sentence_tokens = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n","print(\"Sentence Tokens:\", sentence_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"beB-ATyQralW","executionInfo":{"status":"ok","timestamp":1737359423799,"user_tz":-300,"elapsed":432,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"0be46159-b5d0-4500-ab3c-5bebd8bb5a16"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokens:  ['Natural', 'Language', 'Processing', 'is', 'amazing', 'Let', 's', 'learn', 'tokenization']\n","Sentence Tokens: [\"Natural Language Processing is amazing! Let's learn tokenization.\"]\n"]}]},{"cell_type":"markdown","source":["### 6. Tokenization with Gensim\n"],"metadata":{"id":"hWoGCT7-sYSO"}},{"cell_type":"code","source":["from gensim.utils import tokenize\n","\n","text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","#word tokenization\n","word_tokens=list(tokenize(text))\n","print(\"Word Tokens: \",word_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"McYwoT27sZJZ","executionInfo":{"status":"ok","timestamp":1737359540626,"user_tz":-300,"elapsed":398,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"60a9a28d-a659-46bb-bfda-296e3e27c249"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokens:  ['Natural', 'Language', 'Processing', 'is', 'amazing', 'Let', 's', 'learn', 'tokenization']\n"]}]},{"cell_type":"markdown","source":["### 7. Tokenization with TensorFlow/Keras\n"],"metadata":{"id":"ZXaiJFw1suoA"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import text_to_word_sequence\n","\n","text = \"Natural Language Processing is amazing! Let's learn tokenization.\"\n","\n","# word tokenization\n","word_tokens=text_to_word_sequence(text)\n","print(\"Word Tokens: \",word_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v-2JbH0Nsvxu","executionInfo":{"status":"ok","timestamp":1737359628238,"user_tz":-300,"elapsed":375,"user":{"displayName":"Waleed Usman","userId":"14222602887761143824"}},"outputId":"79a3e8b4-c08a-4409-f8c1-89b196e45c9d"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokens:  ['natural', 'language', 'processing', 'is', 'amazing', \"let's\", 'learn', 'tokenization']\n"]}]},{"cell_type":"markdown","source":["## Comparison of Tokenization Methods\n","\n","| Library/Method        | Features                                                                 |\n","|-----------------------|-------------------------------------------------------------------------|\n","| NLTK                 | Simple, beginner-friendly, supports basic tokenization.                 |\n","| SpaCy                | Fast, efficient, language-specific models, great for production.        |\n","| Hugging Face         | Model-specific tokenization (subword-level, special tokens, etc.).      |\n","| `str.split()`        | Extremely basic, lacks NLP-specific capabilities.                       |\n","| `re` (Regex)         | Fully customizable tokenization.                                        |\n","| Gensim               | Lightweight and useful for topic modeling.                             |\n","| TensorFlow/Keras     | Deep learning-specific workflows.                                       |\n","| TorchText            | PyTorch integration, efficient preprocessing pipelines.                 |\n"],"metadata":{"id":"tiLR_6MntHf4"}}]}