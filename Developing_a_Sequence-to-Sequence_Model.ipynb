{"cells":[{"cell_type":"markdown","metadata":{"id":"f7418e68-fdd5-47e5-95e5-d2a29e86f7ef"},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\">\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","metadata":{"id":"5c27b4ee-3ce4-4000-a7f8-8139d7af5abb"},"source":["# Sequence-to-Sequence RNN Models: Translation Task\n"]},{"cell_type":"markdown","metadata":{"id":"28b6d995-0530-4d41-91d5-32676dab1c16"},"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","metadata":{"id":"1f11f920-bf13-44b6-986d-b00f0f44953d"},"source":["In this hands-on guide, you will explore the fundamentals of sequence-to-sequence models and learn how to implement an RNN-based model for a translation task using PyTorch.\n"]},{"cell_type":"markdown","metadata":{"id":"bca8d627-fd1b-4dd7-87ed-cb1d91d13e63"},"source":["## __Table of Contents__\n","\n","<ol>\n","    <li><a href=\"#Objectives\">Objectives</a></li>\n","    <li>\n","        <a href=\"#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n","            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n","        </ol>\n","    </li>\n","    <li>\n","        <a href=\"#Background\">Background</a>\n","        <ol>\n","            <li><a href=\"#History-of-sequence-to-sequence-models\">History of sequence-to-sequence models</a></li>\n","            <li><a href=\"#Introduction-to-RNNs\">Introduction to RNNs</a></li>\n","            <li><a href=\"#Sequence-to-sequence-architecture\">Sequence-to-sequence architecture</a></li>    \n","        </ol>\n","    <li><a href=\"#Encoder-implementation-in-PyTorch\">Encoder implementation in PyTorch</a></li>\n","    <li><a href=\"#Decoder-implementation-in-PyTorch\">Decoder implementation in PyTorch</a></li>\n","    <li><a href=\"#Sequence-to-sequence-model-implementation-in-PyTorch\">Sequence-to-sequence model implementation in PyTorch</a>\n","    <li><a href=\"#Training-model-in-PyTorch\">Training model in PyTorch</a></li>\n","    <li><a href=\"#Evaluating-model-in-PyTorch\">Evaluating model in PyTorch</a></li>\n","    <li><a href=\"#Data-preprocessing\">Data preprocessing</a></li>\n","    <li><a href=\"#Training-the-model\">Training the model</a></li>\n","    <ol>\n","        <li><a href=\"#Initializations\">Initializations</a></li>\n","        <li><a href=\"#Training\">Training</a></li>\n","    </ol>\n","    <li><a href=\"#Model-inference\">Model inference</a></li>\n","    <li><a href=\"#BLEU-score-metric-for-evaluation\">BLEU score metric for evaluation</a></li>\n","    <li><a href=\"#Exercises\">Exercises</a></li>\n","\n","</ol>\n"]},{"cell_type":"markdown","metadata":{"id":"2c74a1ad-742f-4637-888c-3e772879951c"},"source":["## Objectives\n","After completing this lab, you will be able to:\n","\n"," - Comprehend recurrent neural networks (RNN) architecture\n"," - Create an Encoder-Decoder model for a translation task\n"," - Train and evaluate the model\n"," - Create a generator for the translation task\n"," - Explain concepts related to Perplexity and BLEU score and use them for evaluating translations\n"]},{"cell_type":"markdown","metadata":{"id":"a6f493e6-8e8f-45d4-9465-00de58fb00dd"},"source":["----\n"]},{"cell_type":"markdown","metadata":{"id":"cba92fa1-52d3-44bc-90e3-6a3fe7593e28"},"source":["## Setup\n"]},{"cell_type":"markdown","metadata":{"id":"09096cfc-e764-4d22-a65b-d8b39a9f58f5"},"source":["### Installing required libraries\n","\n","<h2 style=\"color:red;\">After installing the libraries below please RESTART THE KERNEL and run all cells.</h2>\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"154d0ae7-dacf-46c5-94b6-b1e2f77eba57","executionInfo":{"status":"ok","timestamp":1738683916480,"user_tz":-300,"elapsed":214974,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"372b97a1-0371-4eb5-b71e-ff320165830a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torch==2.2.2\n","  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.2.0 (from torch==2.2.2)\n","  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n","Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[0mDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.1.0\n","    Uninstalling triton-3.1.0:\n","      Successfully uninstalled triton-3.1.0\n","  Attempting uninstall: nvidia-nvtx-cu12\n","    Found existing installation: nvidia-nvtx-cu12 12.4.127\n","    Uninstalling nvidia-nvtx-cu12-12.4.127:\n","      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n","  Attempting uninstall: nvidia-nccl-cu12\n","    Found existing installation: nvidia-nccl-cu12 2.21.5\n","    Uninstalling nvidia-nccl-cu12-2.21.5:\n","      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.5.1+cu124\n","    Uninstalling torch-2.5.1+cu124:\n","      Successfully uninstalled torch-2.5.1+cu124\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n","torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n","Collecting torchtext==0.17.2\n","  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n","Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.5.82)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n","Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchtext\n","Successfully installed torchtext-0.17.2\n","Collecting portalocker==2.8.2\n","  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n","Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n","Installing collected packages: portalocker\n","Successfully installed portalocker-2.8.2\n","Collecting torchdata==0.7.1\n","  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.3.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.32.3)\n","Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2024.10.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.19.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.2.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.7.1) (12.5.82)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (2024.12.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n","Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: torchdata\n","Successfully installed torchdata-0.7.1\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Collecting matplotlib==3.9.0\n","  Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting scikit-learn==1.5.0\n","  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (4.55.7)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (2.8.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n","Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scikit-learn, matplotlib\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.6.1\n","    Uninstalling scikit-learn-1.6.1:\n","      Successfully uninstalled scikit-learn-1.6.1\n","  Attempting uninstall: matplotlib\n","    Found existing installation: matplotlib 3.10.0\n","    Uninstalling matplotlib-3.10.0:\n","      Successfully uninstalled matplotlib-3.10.0\n","Successfully installed matplotlib-3.9.0 scikit-learn-1.5.0\n","Collecting numpy==1.26.0\n","  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.26.0\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}],"source":["!pip install torch==2.2.2\n","!pip install torchtext==0.17.2\n","!pip install portalocker==2.8.2\n","!pip install torchdata==0.7.1\n","!pip install pandas\n","!pip install matplotlib==3.9.0 scikit-learn==1.5.0\n","!pip install numpy==1.26.0\n","!pip install spacy\n","!pip install nltk"],"execution_count":1},{"cell_type":"markdown","metadata":{"id":"4e54f463-bb2a-4600-8123-80bf90ad1dea"},"source":["The following required libraries are __not__ pre-installed in the Skills Network Labs environment. __You will need to run the following cell__ to install them:\n"]},{"cell_type":"code","metadata":{"id":"9ccb7d1a-da9f-4ef0-b862-51dd39378d30"},"outputs":[],"source":["!pip install torchtext==0.15.1\n","!pip install torch==2.0.0\n","!pip install spacy==3.7.2\n","!pip install torchdata==0.6.0\n","!pip install portalocker>=2.0.0 #2.7.0\n","!pip install nltk==3.8.1\n","!pip install -U matplotlib\n","\n"],"execution_count":null},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eda5d7d-f6b5-46a4-a208-6c199804ae7b","executionInfo":{"status":"ok","timestamp":1738683966378,"user_tz":-300,"elapsed":18372,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"7b0d81eb-23d5-4f19-f23f-551c3bff1a74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Collecting de-core-news-sm==3.7.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.7.0/de_core_news_sm-3.7.0-py3-none-any.whl (14.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from de-core-news-sm==3.7.0) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.10.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.26.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (1.17.2)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->de-core-news-sm==3.7.0) (0.1.2)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.7.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}],"source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download de_core_news_sm"],"execution_count":2},{"cell_type":"markdown","metadata":{"id":"15d9ef7f-e131-40b2-b600-6e8410f8f913"},"source":["### Importing required libraries\n","\n","_It is recommended that you import all required libraries in one place (here):_\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"94226e40-9908-4ce4-9a40-7a034328160b","executionInfo":{"status":"ok","timestamp":1738684001011,"user_tz":-300,"elapsed":5229,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"efa72a7a-7c9e-4416-9e4f-a53829367eca"},"outputs":[{"output_type":"stream","name":"stderr","text":["Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7fb3cdb50400>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n","    self._make_controller_from_path(filepath)\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1175, in _make_controller_from_path\n","    lib_controller = controller_class(\n","                     ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n","    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n","OSError: dlopen() error\n"]}],"source":["from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torchtext.datasets import multi30k, Multi30k\n","from typing import Iterable, List\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","from torchdata.datapipes.iter import IterableWrapper, Mapper\n","import torchtext\n","from torchtext.vocab import build_vocab_from_iterator\n","from nltk.translate.bleu_score import sentence_bleu\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","\n","import numpy as np\n","import random\n","import math\n","import time\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","\n","\n","# You can also use this section to suppress warnings generated by your code:\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","warnings.filterwarnings('ignore')"],"execution_count":3},{"cell_type":"markdown","metadata":{"id":"a2bd6abb-665f-4e28-a691-23495d2dbe58"},"source":["## Background\n","\n","Sequence-to-sequence (Seq2seq) models have revolutionized various natural language processing (NLP) tasks, such as machine translation, text summarization, and chatbots. These models employ Recurrent Neural Networks (RNNs) to process variable-length input sequences and generate variable-length output sequences.\n"]},{"cell_type":"markdown","metadata":{"id":"66486c77-b704-47d3-bfe6-b8695708a994"},"source":["### History of sequence-to-sequence models\n","\n","Sequence-to-sequence models were introduced as an extension of traditional feedforward neural networks.\n","Researchers realized the need for models that could handle variable-length input and output sequences, such as machine translation.\n","The pioneering work of Sutskever et al. (2014) introduced the use of RNNs for seq2seq models.\n","\n","Here are some main objectives of seq2seq models:\n","- Translation: Translating a sequence from one domain to another (e.g., English to French).\n","- Question answering: Generating a natural language response given an input sentence (e.g., chatbots).\n","- Summarization: Summarizing a long document into a shorter sequence of sentences.\n","And many more applications that involve sequence generation.\n"]},{"cell_type":"markdown","metadata":{"id":"c256a2d4-bf0d-4de4-845b-a7cca2508cde"},"source":["### Introduction to RNNs\n","\n","RNNs are a class of neural networks designed to process sequential data.\n","They maintain an internal memory($h_t$) to capture information from previous steps and use it for current predictions.\n","RNNs have a recurrent connection that allows information to flow from one step to the next.\n","Recurrent Neural Networks (RNNs) operate on sequences and utilize previous states to influence the current state. Here's the general formulation of a simple RNN:\n","\n","\n","Given:\n","\n","-$ \\mathbf{x}_t $: input vector at time step $t$\n","\n","-$ \\mathbf{h}_{t-1} $: hidden state vector from the previous time step\n","\n","-$ \\mathbf{W}_x $ and $ \\mathbf{W}_h $: weight matrices for the input and hidden state, respectively\n","\n","-$ \\mathbf{b} $: bias vector\n","\n","-$\\sigma$: activation function (often a sigmoid or tanh)\n","\n","The update equations for the hidden state $ \\mathbf{h}_t $ and the output $ \\mathbf{y}_t $ are as follows:\n","\n","$$\n","\\begin{align*}\n","\\mathbf{h}_t &= \\sigma(\\mathbf{W}_x \\cdot \\mathbf{x}_t + \\mathbf{W}_h \\cdot \\mathbf{h}_{t-1} + \\mathbf{b})\n","\\end{align*}\n","$$\n","\n","It can be seen that the hidden state function depends on the previous hidden state as well as the input at time t, which is why it has a collective memory of previous time steps.\n","\n","For the output (if you're making a prediction at each time step):\n","\n","$$\n","\\begin{align*}\n","\\mathbf{y}_t &= \\text{softmax}(\\mathbf{W}_o \\cdot \\mathbf{h}_t + \\mathbf{b}_o)\n","\\end{align*}\n","$$\n","\n","Where:\n","\n","$ \\mathbf{W}_o $: weight matrix for the output AND $ \\mathbf{b}_o$: bias vector for the output\n","\n","\n","\n","Depending on the specific task, an RNN cell can either produce an output from $h_t$ or solely transfer it to the succeeding cell, serving as internal memory. While the architecture's ability to retain memory might seem elusive at first glance, let's elucidate this by implementing a simple RNN to handle the following data mechanism:\n","\n","![a title](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Screenshot%202023-10-19%20at%2011.29.23%E2%80%AFAM.png)\n","\n","\n","The diagram showcases a state machine or transition model with three distinct states, depicted by the prominent purple circles. Each state is distinctly labeled with a value for $ h $: $ h = -1 $, $ h = 0 $, and $ h = 1 $.\n","\n","1. **State $ h = -1 $**:\n","   - Maintains itself when $ x = 1 $ (illustrated by the yellow loop).\n","   - Proceeds to the $ h = 0$ state upon receiving $ x = -1$ (highlighted by the red arrow).\n","\n","2. **State $ h = 0 $**:\n","   - Moves to the $h = -1 $ state when $ x = 1$ (illustrated by the red arrow).\n","   - Advances to the $ h = 1 $ state with $ x = -1$ (marked by the red arrow).\n","\n","3. **State $h = 1 $**:\n","   - Sustains its position when $ x = -1 $ (indicated by the yellow loop).\n","   - Transitions to the $ h = 0 $ state upon receiving $ x = 1 $ (signified by the red arrow).\n","\n","To encapsulate, the diagram effectively portrays transitions among three states based on the input $ x $. Contingent on the prevailing state and the input $ x $, the state machine either transitions to a different state or remains stationary.\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"0ef297ac-19fc-403d-b1f1-167afc6c8544"},"source":["\n","You can represent the previously mentioned state machine using the layer detailed below. Use $tanh$ as the $h$ value should fall between [-1, 1]. Note that you have excluded the output for simplification:\n","\n","$$\\begin{align*}\n","W_{xh} & = -10.0 \\\\\\\\\\\\\\\\\\\\\\\\\\\\\n","W_{hh} & = 10.0 \\\\\n","b_h & = 0.0 \\\\\n","x_t & = 1 \\\\\n","h_{\\text{prev}} & = 0.0 \\\\\n","h_t & = \\tanh(x_t \\cdot W_{xh} + h_{\\text{prev}} \\cdot W_{hh} + b_h)\n","\\end{align*}$$\n"]},{"cell_type":"code","metadata":{"id":"04136ae0-d9fb-4da9-aeb5-f82509b93db2","executionInfo":{"status":"ok","timestamp":1738684019465,"user_tz":-300,"elapsed":978,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}}},"outputs":[],"source":[" W_xh=torch.tensor(-10.0)\n"," W_hh=torch.tensor(10.0)\n"," b_h=torch.tensor(0.0)\n"," x_t=1\n"," h_prev=torch.tensor(-1)"],"execution_count":4},{"cell_type":"markdown","metadata":{"id":"f9fea4a1-ce05-484e-ba0a-364de5f91b8f"},"source":["Consider the following sequence $x_t$ for  $t=0,1,..,7$,\n"]},{"cell_type":"code","metadata":{"id":"9a2bb1a8-3a74-4987-929b-17dedd0e184f","executionInfo":{"status":"ok","timestamp":1738684024764,"user_tz":-300,"elapsed":591,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}}},"outputs":[],"source":["X=[1,1,-1,-1,1,1]"],"execution_count":5},{"cell_type":"markdown","metadata":{"id":"e5b1bdf0-1f63-44bd-8766-8fe9c717d03a"},"source":["Assuming that you start from the intial state $h = 0$,  with the above input vector $x$, the state vector $h$ should look like this:\n"]},{"cell_type":"code","metadata":{"id":"02869864-66c3-4581-bd3a-27130084a510","executionInfo":{"status":"ok","timestamp":1738684029903,"user_tz":-300,"elapsed":1102,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}}},"outputs":[],"source":["H=[-1,-1,0,1,0,-1]"],"execution_count":6},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a60e5b8b-5ce8-47a0-8e98-4e2cdf21bf8a","executionInfo":{"status":"ok","timestamp":1738684036510,"user_tz":-300,"elapsed":16,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"6adb564d-3a97-47af-802d-1b8e81cbe5a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["t= 1\n","h_t-1 -1\n","x_t 1\n","h_t -1.0\n","\n","\n","t= 2\n","h_t-1 -1.0\n","x_t 1\n","h_t -1.0\n","\n","\n","t= 3\n","h_t-1 -1.0\n","x_t -1\n","h_t 0.0\n","\n","\n","t= 4\n","h_t-1 0.0\n","x_t -1\n","h_t 1.0\n","\n","\n","t= 5\n","h_t-1 1.0\n","x_t 1\n","h_t 0.0\n","\n","\n","t= 6\n","h_t-1 0.0\n","x_t 1\n","h_t -1.0\n","\n","\n"]}],"source":["# Initialize an empty list to store the predicted state values\n","H_hat = []\n","# Loop through each data point in the input sequence X\n","t=1\n","for x in X:\n","    # Assign the current data point to x_t\n","    print(\"t=\",t)\n","    x_t = x\n","    # Print the value of the previous state (h at time t-1)\n","    print(\"h_t-1\", h_prev.item())\n","\n","    # Compute the current state (h at time t) using the RNN formula with tanh activation\n","    h_t = torch.tanh(x_t * W_xh + h_prev * W_hh + b_h)\n","\n","    # Update h_prev to the current state value for the next iteration\n","    h_prev = h_t\n","\n","    # Print the current input value (x at time t)\n","    print(\"x_t\", x_t)\n","\n","    # Print the computed state value (h at time t)\n","    print(\"h_t\", h_t.item())\n","    print(\"\\n\")\n","\n","    # Append the current state value to the H_hat list after converting it to integer\n","    H_hat.append(int(h_t.item()))\n","    t+=1\n"],"execution_count":7},{"cell_type":"markdown","metadata":{"id":"23751479-1442-4b69-b4c0-d27c2c9e9723"},"source":["\n","\n","You can evaluate the accuracy of the predicted state ```H_hat``` by comparing it to the actual state ```H```. In RNNs, the state $ h_t $ is utilized to predict an output sequence $y_t $ based on the given input sequence $ x_t $.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"68aa6a7c-035c-46f3-a1ff-f2caeb1a9338","executionInfo":{"status":"ok","timestamp":1738684054568,"user_tz":-300,"elapsed":963,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"762540df-5d9d-4646-8fe6-96faad5576f8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-1, -1, 0, 1, 0, -1]"]},"metadata":{},"execution_count":8}],"source":["H_hat"],"execution_count":8},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e6b575c4-43c9-49ce-8e92-77ba9906e817","executionInfo":{"status":"ok","timestamp":1738684058598,"user_tz":-300,"elapsed":11,"user":{"displayName":"Waleed Usman","userId":"03064618890249256362"}},"outputId":"6dc07a4a-0de6-46a4-f352-e3fe7cc96f9b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-1, -1, 0, 1, 0, -1]"]},"metadata":{},"execution_count":9}],"source":["H"],"execution_count":9},{"cell_type":"markdown","metadata":{"id":"70a5cd2b-e428-4d30-8631-4693ae366a22"},"source":["While you have pre-defined the $W_{xh}$ and $W_{hh}$  and $b_h$, in practice these values need to be identified through training on data.\n"]},{"cell_type":"markdown","metadata":{"id":"2f46ad94-9b71-4e2d-bae1-b1e96cf139b3"},"source":["In practice, modifications and enhancements, such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), are often used to address issues like the vanishing gradient problem in basic RNNs.\n"]},{"cell_type":"markdown","metadata":{"id":"7825d58d-7cd5-47f0-bf01-038f4f0f9e7c"},"source":["An LSTM cell has three main components: an input gate, a forget gate, and an output gate.\n","- The **input gate** controls how much new information should be stored in the cell's memory. It looks at the current input and the previous hidden state and decides which parts of the new input to remember.\n","- The **forget gate** determines what information should be discarded or forgotten from the cell's memory. It considers the current input and the previous hidden state and decides which parts of the previous memory are no longer relevant.\n","- The **output gate** determines what information should be outputted from the cell. It looks at the current input and the previous hidden state and decides which parts of the cell's memory to include in the output.\n","\n","The key idea behind LSTM cells is that they have a separate memory state that can selectively retain or forget information over time. This helps them handle long-range dependencies and remember important information from earlier steps in a sequence.\n"]},{"cell_type":"markdown","metadata":{"id":"b722a096-431d-4972-91d9-a5610d76a621"},"source":["### Sequence-to-sequence architecture\n","\n","Seq2seq models have an Encoder-Decoder structure. The encoder encodes the input sequence into a fixed-dimensional representation, often called the context vector($h_t$). The decoder generates the output sequence based on the encoded context vector.\n"]},{"cell_type":"markdown","metadata":{"id":"86b0c233-5457-421c-a824-410bc27300b8"},"source":["Let's look closer into the encoder and decoder boxes in the video below. Translation is a typical sequence-to-sequence task. The input is a sequence of words in the original language(\"I love to travel\"), while output is its translation in the destination language(\"J'adore voyager\"). As shown in the video, input is fed into the decoder part, one word after another. Each RNN cell receives a word($x_t$) and has an internal memory($h_t$). After processing the input and $h_t$, RNN cell passes an updated context vector($h_{t+1}$) to the next RNN cell. When the end of sentence is reached, the context vector is passed to the decoder part. Decoder cells are also RNN cells that receive context vector and generate the output word by word. Each RNN receives the generated word as well as the updated context vector from its previous cell and generates the next word($y_t$). This architecture allows for generating text without length restrictions.\n"]},{"cell_type":"markdown","metadata":{"id":"350824bf-f95f-4155-af4a-a617ebb7688e"},"source":["<video width=\"640\" height=\"480\"\n","src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Translation_RNN.mp4\"\n","controls>\n","</video>\n"]},{"cell_type":"markdown","metadata":{"id":"234acdf1-c32c-4da6-b1ff-bceb125a1768"},"source":["## Encoder implementation in PyTorch\n","\n","To implement the encoder part using Pytorch, you will create the sub-class of the torch.nn.Module class and define the __init__() and __forward__() method.\n","\n","Let's first define the parameters that are used in __init__() function:\n","- The `vocab_len` is nothing but the number of unique words present in the vocabulary. After pre-processing the data, you can count the number of unique words in your vocabulary and use that count here. This will be the dimension of the model input.\n","- The embedding_dim is the output dimension of the embedding vector you need. A good practice is to use 256-512 for sample demo app like you are building here.\n","- LSTM can indeed be stacked, allowing for multiple layers. In the initial implementation, you will use only one layer. However, to accommodate future flexibility, you will pass the parameter `n_layers` to specify the number of layers in the LSTM.\n","- `hid_dim` is the dimensionality of the hidden and cell states.\n","- `dropout` is the amount of dropout to use. This is a regularization parameter to prevent overfitting.\n","\n","Now, let's look into the layers:\n","- The Embedding layer takes the input data and outputs the embedding vector, hence the dimension of those needs to be defined as `vocab_len` and `embedding_dim`.\n","- The LSTM Layer takes the `embedding_dim` as the input data and creates total 3 outputs: `hidden`, `cell` and `output`. Here you need to define the number of neurons you need in LSTM, which is defined using the `hid_dim`.\n","\n","\n","In the __forward__() function, the Embedding layer is defined that utilizes the `vocab_len` to internally convert the input_batch into a one-hot representation. Next, the LSTM layer receives the embedded input and outputs three vectors: Output, Hidden and cell. As for the encoder, you don't require the output vector from the LSTM as you only pass the context vector(`hidden`+`cell`) to the decoder block. Therefore, forward() only returns hidden and cell.\n","\n","Note: When using an LSTM, you have an additional cell state. However, if you were using a GRU, you would only have the hidden state.\n"]},{"cell_type":"code","metadata":{"id":"dabd30a1-787e-4ae9-ab12-3be8fe6b48d9"},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self, vocab_len, emb_dim, hid_dim, n_layers, dropout_prob):\n","        super().__init__()\n","\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","\n","        self.embedding = nn.Embedding(vocab_len, emb_dim)\n","\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout_prob)\n","        self.dropout = nn.Dropout(dropout_prob)\n","\n","    def forward(self, input_batch):\n","        #input_batch = [src len, batch size]\n","        embed = self.dropout(self.embedding(input_batch))\n","        embed = embed.to(device)\n","        #outputs = [src len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","        outputs, (hidden, cell) = self.lstm(embed)\n","\n","        return hidden, cell\n"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"1dd47366-9358-4ba6-9c7d-2c4aa61eee2c"},"source":["Now you are ready to create an encoder instance to see how it works:\n"]},{"cell_type":"code","metadata":{"id":"67e15c4d-ab9a-41e8-bb8f-6e72304dcced"},"outputs":[],"source":["vocab_len = 8\n","emb_dim = 10\n","hid_dim=8\n","n_layers=1\n","dropout_prob=0.5\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","encoder_t = Encoder(vocab_len, emb_dim, hid_dim, n_layers, dropout_prob).to(device)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"e6e3f8b7-dd73-4eb1-b0ec-79ad3bed474c"},"source":["Let's see a simple example where the encoder forward method transforms the `src` sentence into a `hidden` and `cell` states. tensor([[0],[3],[4],[2],[1]]) is equal to `src` = 0,3,4,2,1 in which each number represents a token in the `src` vocabulary. For instance, 0:`<bos>`,3:\"Das\", 4:\"ist\",2:\"schön\", 1:`<eos>`. Note that here you have batch size of 1.\n"]},{"cell_type":"code","metadata":{"id":"c0fc5fbf-f48e-45e1-836b-13289ac91cd5"},"outputs":[],"source":["src_batch = torch.tensor([[0,3,4,2,1]])\n","# you need to transpose the input tensor as the encoder LSTM is in Sequence_first mode by default\n","src_batch = src_batch.t().to(device)\n","print(\"Shape of input(src) tensor:\", src_batch.shape)\n","hidden_t , cell_t = encoder_t(src_batch)\n","print(\"Hidden tensor from encoder:\",hidden_t ,\"\\nCell tensor from encoder:\", cell_t)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"5b2fdd99-c617-4039-af11-6f4c93bfe315"},"source":["The encoder takes the entire source sequence as input, which consists of a sequence of words or tokens. The encoder LSTM processes the entire input sequence and updates its hidden states at each time step. The hidden states of the LSTM network act as a form of memory and capture the contextual information of the input sequence. After processing the entire input sequence, the final hidden state of the encoder LSTM captures the summarized representation of the input sequence's context. This final hidden state is sometimes referred to as the \"context vector\".\n"]},{"cell_type":"markdown","metadata":{"id":"099257ba-084e-4dd5-9511-07b7479aa22b"},"source":["## Decoder implementation in PyTorch\n","\n","To have a better understanding of the internal mechanism of the decoder part, let's take a closer look into it:\n"]},{"cell_type":"markdown","metadata":{"id":"c0f57bbd-c26c-45c6-9315-bf7029595806"},"source":["<video width=\"640\" height=\"480\"\n","       src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/decoder_RNN.mp4\"\n","       controls>\n","</video>\n"]},{"cell_type":"markdown","metadata":{"id":"f6849bd7-7938-4082-ae4d-8e2a94ae79c9"},"source":["The decoder class inherits from nn.Module, which is a base class for all neural network modules in PyTorch.\n","The constructor (__init__ method) initializes the parameters and layers of the decoder.\n","- `output_dim` is the number of possible output values(target vocab length).\n","- `emb_dim` is the dimensionality of the embedding layer.\n","- `hid_dim` is the dimensionality of the hidden state in the LSTM.\n","- `n_layers` is the number of layers in the LSTM.\n","- `dropout` is the dropout probability.\n","\n","The decoder contains the following layers:\n","- `embedding`: An embedding layer that maps the output values to dense vectors of size emb_dim.\n","- `lstm`: An LSTM layer that takes the embedded input and produces hidden states of size hid_dim.\n","-  `fc_out`: A linear layer that maps the LSTM output to the output dimension output_dim.\n","- `softmax`: A log-softmax activation function applied to the output to obtain a probability distribution over the output values.\n","- `dropout`: A dropout layer that applies dropout to the embedded input.\n"]},{"cell_type":"code","metadata":{"id":"5b9d047a-ae52-4498-a207-828c07775755"},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","\n","        self.output_dim = output_dim\n","        self.hid_dim = hid_dim\n","        self.n_layers = n_layers\n","\n","\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        self.softmax = nn.LogSoftmax(dim=1)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, cell):\n","\n","\n","        #input = [batch size]\n","\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","\n","        #n directions in the decoder will both always be 1, therefore:\n","        #hidden = [n layers, batch size, hid dim]\n","        #context = [n layers, batch size, hid dim]\n","\n","        input = input.unsqueeze(0)\n","        #input = [1, batch size]\n","\n","        embedded = self.dropout(self.embedding(input))\n","        #embedded = [1, batch size, emb dim]\n","\n","        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n","        #output = [seq len, batch size, hid dim * n directions]\n","        #hidden = [n layers * n directions, batch size, hid dim]\n","        #cell = [n layers * n directions, batch size, hid dim]\n","\n","        #seq len and n directions will always be 1 in the decoder, therefore:\n","        #output = [1, batch size, hid dim]\n","        #hidden = [n layers, batch size, hid dim]\n","        #cell = [n layers, batch size, hid dim]\n","        prediction_logit = self.fc_out(output.squeeze(0))\n","        prediction = self.softmax(prediction_logit)\n","        #prediction = [batch size, output dim]\n","\n","\n","        return prediction, hidden, cell"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"c52bad40-1242-4c5f-8448-824496a019be"},"source":["You can create a decoder instance. The output dimension is set as the target vocab length.\n"]},{"cell_type":"code","metadata":{"id":"c5927ed5-d227-45b4-a101-ac42c49b3fe2"},"outputs":[],"source":["output_dim = 6\n","emb_dim=10\n","hid_dim = 8\n","n_layers=1\n","dropout=0.5\n","decoder_t = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout).to(device)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"2365df66-d882-4008-a69a-5f53c984e471"},"source":["Now that you have instances of both encoder and decoder, you are ready to connect them (the red box in the diagram below). First, let's see how you can pass the Hidden and Cell (the pink cell within the red box) from encoder (the green boxes container) to decoder (the orange boxes container). Looking at the diagram, you can see that the decoder also receives an input which is the previous word that it has predicted. For the first decoder cell, this input is `<bos>` token. Each decoder cell outputs a prediction and updates the cell and state to pass to the next decoder cell. prediction is a probability distribution over possible target tokens (length of target vocab).\n","\n","![connection](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/ED_connection.JPG)\n"]},{"cell_type":"code","metadata":{"id":"abe36cb4-98db-4716-8921-180254fc5013"},"outputs":[],"source":["input_t = torch.tensor([0]).to(device) #<bos>\n","input_t.shape\n","prediction, hidden, cell = decoder_t(input_t, hidden_t , cell_t)\n","print(\"Prediction:\", prediction, '\\nHidden:',hidden,'\\nCell:', cell)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"b9a0d66e-590b-4290-89e7-f3f3c9b852d2"},"source":["# Encoder-decoder connection\n"]},{"cell_type":"markdown","metadata":{"id":"33051a24-4ea4-4a01-961c-4239d4612732"},"source":["Alright! You learned how to create encoder and decoder modules and how to pass input to them. Now you need to create the connection so that the model can process (`src`,`trg`) pairs and generate the translation. suppose that `trg` is tensor ([[0],[2],[3],[5],[1]]) which is equal to sequence 0,2,3,5,1 in which each number represents a token in the target vocabulary. For instance, 0:`<bos>`,2:\"this\", 3:\"is\",5:\"beautiful\", 1:`<eos>`.\n"]},{"cell_type":"code","metadata":{"id":"0addb152-fddc-4da2-949f-38d7b680b4a9"},"outputs":[],"source":["\n","#trg = [trg len, batch size]\n","#teacher_forcing_ratio is probability to use teacher forcing\n","#e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n","teacher_forcing_ratio = 0.5\n","trg = torch.tensor([[0],[2],[3],[5],[1]]).to(device)\n","\n","\n","batch_size = trg.shape[1]\n","trg_len = trg.shape[0]\n","trg_vocab_size = decoder_t.output_dim\n","\n","#tensor to store decoder outputs\n","outputs_t = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n","\n","#send to device\n","\n","hidden_t = hidden_t.to(device)\n","cell_t = cell_t.to(device)\n","\n","\n","#first input to the decoder is the <bos> tokens\n","input = trg[0,:]\n","\n","\n","for t in range(1, trg_len):\n","\n","    #you loop through the trg len and generate tokens\n","    #decoder receives previous generated token, cell and hidden\n","    # decoder outputs it prediction(probablity distribution for the next token) and updates hidden and cell\n","    output_t, hidden_t, cell_t = decoder_t(input, hidden_t, cell_t)\n","\n","    #place predictions in a tensor holding predictions for each token\n","    outputs_t[t] = output_t\n","\n","    #decide if you are going to use teacher forcing or not\n","    teacher_force = random.random() < teacher_forcing_ratio\n","\n","    #get the highest predicted token from your predictions\n","    top1 = output_t.argmax(1)\n","\n","\n","    #if teacher forcing, use actual next token as next input\n","    #if not, use predicted token\n","    #input = trg[t] if teacher_force else top1\n","    input = trg[t] if teacher_force else top1\n","\n","print(outputs_t,outputs_t.shape )"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"7158369f-e3bd-472b-8f8c-7b40b3f47b94"},"source":["The size of output tensor is (trg_len, batch_size, trg_vocab_size). This is because for each `trg` token (length of `trg`) the model outputs a probability distribution over all possible tokens(trg vocab length). Therefore, to generate the predicted tokens or translation of the `src` sentence, you need to get the maximum probability for each token:\n"]},{"cell_type":"code","metadata":{"id":"c0371be9-1bb6-4876-9e87-c50d1bc1c8b8"},"outputs":[],"source":["# Note that you need to get the argmax from the second dimension as **outputs** is an array of **output** tensors\n","pred_tokens = outputs_t.argmax(2)\n","print(pred_tokens)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"9459d4e9-241c-43ef-9f72-2bc4988009e4"},"source":["It is no surprise that the translation is not correct (trg = tensor([[0],[2],[3],[5],[1]]) as the model has not yet gone through any training.\n"]},{"cell_type":"markdown","metadata":{"id":"d28ada82-3f83-4f5b-8a96-b7fd5be8da7f"},"source":["Let's put together all the code for connecting the encoder and decoder in a seq2seq class for better usability.\n"]},{"cell_type":"markdown","metadata":{"id":"30ae6f22-93af-40ec-aefd-e8a4035926db"},"source":["## Sequence-to-sequence model implementation in PyTorch\n","Let's connect encoder and decoder components to create the seq2seq model.\n","\n","You define the seq2seq class that inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n","Inputs are:\n","- `encoder` and `decoder` are instances of the encoder and decoder networks that you have already defined.\n","- `device` specifies the device (e.g., CPU or GPU) on which the computations will be performed.\n","- `trg_vocab` represents the vocabulary of the target language. It is used to determine the size of the output vocabulary.\n","\n","**forward** method defines the forward pass of the seq2seq model. It takes three arguments: `src`, `trg`, and `teacher_forcing_ratio`.:\n","\n","- `src` represents the source sequences, and `trg` represents the target sequences.\n","- `teacher_forcing_ratio` is a probability that determines whether teacher forcing will be used during training only. Teacher forcing is a technique where the true target sequence is fed as input to the decoder at each time step, instead of using the predicted output from the previous time step.\n","\n","The **forward** method initializes some variables needed for the forward pass, such as `batch_size`, `trg_len`, and `trg_vocab_size`. It also creates an empty tensor called `outputs` to store the decoder outputs for each time step.\n","\n","The `hidden` and `cell` states of the encoder are obtained by calling the encoder (src) method. These states are then used as the initial states for the decoder.\n","\n","The input to the decoder at the first time step is the <bos> token of the target sequences.\n","\n","The decoder is iterated over for each time step in the target sequences (`for t in range(1, trg_len)`). The input, along with the previous hidden and cell states, is passed to the decoder, and it produces an output tensor. The `output` tensor is stored in the `outputs` tensor.\n","\n","At each time step, there is a decision made whether to use teacher forcing or not based on the teacher_forcing_ratio probability. If teacher forcing is used, the true next token from the target sequences (`trg[t]`) is used as the input for the next time step. Otherwise, the predicted token from the previous time step (`top1 = output.argmax(1)`) is used.\n","\n","Finally, the `outputs` tensor containing the predicted outputs for each time step is returned.\n"]},{"cell_type":"code","metadata":{"id":"da4891fe-fc90-495d-b3c5-d651cc55ab63"},"outputs":[],"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device,trg_vocab):\n","        super().__init__()\n","\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        self.trg_vocab = trg_vocab\n","\n","        assert encoder.hid_dim == decoder.hid_dim, \\\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\n","        assert encoder.n_layers == decoder.n_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","\n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        #src = [src len, batch size]\n","        #trg = [trg len, batch size]\n","        #teacher_forcing_ratio is probability to use teacher forcing\n","        #e.g. if teacher_forcing_ratio is 0.75 you use ground-truth inputs 75% of the time\n","\n","\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","\n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src)\n","        hidden = hidden.to(device)\n","        cell = cell.to(device)\n","\n","\n","        #first input to the decoder is the <bos> tokens\n","        input = trg[0,:]\n","\n","        for t in range(1, trg_len):\n","\n","            #insert input token embedding, previous hidden and previous cell states\n","            #receive output tensor (predictions) and new hidden and cell states\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","\n","            #place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","\n","            #decide if you are going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","\n","            #get the highest predicted token from your predictions\n","            top1 = output.argmax(1)\n","\n","\n","            #if teacher forcing, use actual next token as next input\n","            #if not, use predicted token\n","            #input = trg[t] if teacher_force else top1\n","            input = trg[t] if teacher_force else top1\n","\n","\n","        return outputs"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"3bf5af32-448a-484d-a0d1-b241bcd33b0f"},"source":["## Training model in PyTorch\n","Now that the model is defined, you define a train function the seq2seq model. Let's go through the code and understand its components:\n","\n","1. `train(model, iterator, optimizer, criterion, clip)` takes five arguments:\n","\n","   - `model` is the model that will be trained.\n","   - `iterator` is an iterable object that provides the training data in batches.\n","   - `optimizer` is the optimization algorithm used to update the model's parameters.\n","   - `criterion` is the loss function that measures the model's performance.\n","   - `clip` is a value used to clip the gradients to prevent them from becoming too large during backpropagation.\n","\n","2. The function starts by setting the model to training mode with `model.train()`. This is necessary to enable certain layers (e.g., dropout) that behave differently during training and evaluation.\n","\n","3. It initializes a variable `epoch_loss` to keep track of the accumulated loss during the epoch.\n","\n","4. The function iterates over the training data provided by the `iterator`. Each iteration retrieves a batch of input sequences (`src`) and target sequences (`trg`).\n","\n","5. The input sequences (`src`) and target sequences (`trg`) are moved to the appropriate device (e.g., GPU) using `src = src.to(device)` and `trg = trg.to(device)`.\n","\n","6. The gradients of the model's parameters are cleared using `optimizer.zero_grad()` to prepare for the new batch.\n","\n","7. The model is then called with `output = model(src, trg)` to obtain the model's predictions for the target sequences.\n","\n","8. The `output` tensor has dimensions `[trg len, batch size, output dim]`. To calculate the loss, the tensor is reshaped to `[trg len - 1, batch size, output dim]` to remove the initial `<bos>` token, which is not used for calculating the loss.\n","\n","9. The target sequences (`trg`) are also reshaped to `[trg len - 1]` by removing the initial `<bos>` token and making it a contiguous tensor. This matches the shape of the reshaped `output` tensor.\n","\n","10. The loss between the reshaped `output` and `trg` tensors is calculated using the specified `criterion`.\n","\n","11. The gradients of the loss with respect to the model's parameters are computed using `loss.backward()`.\n","\n","12. The gradients are then clipped to a maximum value specified by `clip` using `torch.nn.utils.clip_grad_norm_(model.parameters(), clip)`. This prevents the gradients from becoming too large, which can cause issues during optimization.\n","\n","13. The optimizer's `step()` method is called to update the model's parameters using the computed gradients.\n","\n","14. The current batch loss (`loss.item()`) is added to the `epoch_loss` variable.\n","\n","15. After all the batches have been processed, the function returns the average loss per batch for the entire epoch, calculated as `epoch_loss / len(list(iterator))`.\n"]},{"cell_type":"code","metadata":{"id":"3375fdaf-20d4-4fd8-afbf-d0ea6c4c13ed"},"outputs":[],"source":["def train(model, iterator, optimizer, criterion, clip):\n","\n","    model.train()\n","\n","    epoch_loss = 0\n","\n","    # Wrap iterator with tqdm for progress logging\n","    train_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n","\n","    for i, (src,trg) in enumerate(iterator):\n","\n","        src = src.to(device)\n","        trg = trg.to(device)\n","        optimizer.zero_grad()\n","\n","        output = model(src, trg)\n","\n","        #trg = [trg len, batch size]\n","        #output = [trg len, batch size, output dim]\n","\n","        output_dim = output.shape[-1]\n","\n","        output = output[1:].view(-1, output_dim)\n","\n","        trg = trg[1:].contiguous().view(-1)\n","\n","        #trg = [(trg len - 1) * batch size]\n","        #output = [(trg len - 1) * batch size, output dim]\n","\n","        loss = criterion(output, trg)\n","\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","\n","        optimizer.step()\n","\n","        # Update tqdm progress bar with the current loss\n","        train_iterator.set_postfix(loss=loss.item())\n","\n","        epoch_loss += loss.item()\n","\n","\n","    return epoch_loss / len(list(iterator))"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"54332ef1-5dba-4d7c-86fa-a325210d71d1"},"source":["## Evaluating model in PyTorch\n","You also need to define a function to evaluate the model. Let's go through the code and understand its components:\n","\n","1. `evaluate(model, iterator, criterion)` takes three arguments:\n","   - `model` is the neural network model that will be evaluated.\n","   - `iterator` is an iterable object that provides the evaluation data in batches.\n","   - `criterion` is the loss function that measures the model's performance.\n","* Note that evaluate function do not perform any optimization on the model.\n","\n","2. The function starts by setting the model to evaluation mode with `model.eval()`.\n","\n","3. It initializes a variable `epoch_loss` to keep track of the accumulated loss during the evaluation.\n","\n","4. The function enters a `with torch.no_grad()` block, which ensures that no gradients are computed during the evaluation. This saves memory and speeds up the evaluation process since gradients are not needed for parameter updates.\n","\n","5. The function iterates over the evaluation data provided by the `iterator`. Each iteration retrieves a batch of input sequences (`src`) and target sequences (`trg`).\n","\n","6. The input sequences (`src`) and target sequences (`trg`) are moved to the appropriate device (e.g., GPU) using `src = src.to(device)` and `trg = trg.to(device)`.\n","\n","7. The model is then called with `output = model(src, trg, 0)` to obtain the model's predictions for the target sequences. The third argument `0` is passed to indicate that teacher forcing is turned off during evaluation.  During evaluation, teacher forcing is typically turned off to evaluate the model's ability to generate sequences based on its own predictions.\n","\n","8. The `output` tensor has dimensions `[trg len, batch size, output dim]`. To calculate the loss, the tensor is reshaped to `[trg len - 1, batch size, output dim]` to remove the initial `<bos>` (beginning of sequence) token, which is not used for calculating the loss.\n","\n","9. The target sequences (`trg`) are also reshaped to `[trg len - 1]` by removing the initial `<bos>` token and making it a contiguous tensor. This matches the shape of the reshaped `output` tensor.\n","\n","10. The loss between the reshaped `output` and `trg` tensors is calculated using the specified `criterion`.\n","\n","11. The current batch loss (`loss.item()`) is added to the `epoch_loss` variable.\n","\n","12. After all the batches have been processed, the function returns the average loss per batch for the entire evaluation, calculated as `epoch_loss / len(list(iterator))`.\n"]},{"cell_type":"code","metadata":{"id":"196576f1-caa0-47b8-b0c7-24d268de79a7"},"outputs":[],"source":["def evaluate(model, iterator, criterion):\n","\n","    model.eval()\n","\n","    epoch_loss = 0\n","\n","    # Wrap iterator with tqdm for progress logging\n","    valid_iterator = tqdm(iterator, desc=\"Training\", leave=False)\n","\n","    with torch.no_grad():\n","\n","        for i, (src,trg) in enumerate(iterator):\n","\n","            src = src.to(device)\n","            trg = trg.to(device)\n","\n","            output = model(src, trg, 0) #turn off teacher forcing\n","\n","            #trg = [trg len, batch size]\n","            #output = [trg len, batch size, output dim]\n","\n","            output_dim = output.shape[-1]\n","\n","            output = output[1:].view(-1, output_dim)\n","\n","            trg = trg[1:].contiguous().view(-1)\n","\n","\n","            #trg = [(trg len - 1) * batch size]\n","            #output = [(trg len - 1) * batch size, output dim]\n","\n","            loss = criterion(output, trg)\n","            # Update tqdm progress bar with the current loss\n","            valid_iterator.set_postfix(loss=loss.item())\n","\n","            epoch_loss += loss.item()\n","\n","    return epoch_loss / len(list(iterator))"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"35dafef9-b9f9-43ec-8034-ce11085043a5"},"source":["## Data preprocessing\n"]},{"cell_type":"markdown","metadata":{"id":"dc068e5a-71b6-466d-8108-b1aebfc1dcc3"},"source":["In this section, you will fetch a language translation dataset called Multi30k, collate it (tokenization, numericalization, and adding BOS/EOS and padding) and create iterable batches of src and trg tensors.\n","\n","This leverages the predefined collate_fn to efficiently curate and ready batches for training the transformer model. The primary aim is to delve deeper into the intricacies of the RNN encoder and decoder components.\n"]},{"cell_type":"markdown","metadata":{"id":"cc99b253-e9c8-4ae6-b3ce-05983649974c"},"source":["A \"Multi30K_de_en_dataloader.py\" file has been created that contains all the transformation processes on data. Here, you only download the file:\n"]},{"cell_type":"code","metadata":{"id":"d7639c72-dfd2-4809-be80-0ed291232bbf"},"outputs":[],"source":["!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/Multi30K_de_en_dataloader.py'"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"100b983f-a6cb-4165-8e80-412f9f69ca11"},"source":["Let's run it:\n"]},{"cell_type":"code","metadata":{"id":"7a9c94d4-6ee5-4165-b533-08dd9d80f749"},"outputs":[],"source":["%run Multi30K_de_en_dataloader.py"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"7b0f245a-4b36-499d-9692-66e65a42eb9d"},"source":["There you go! You only need to call the function `get_translation_dataloaders(batch_size = N,flip=True)` with an arbitrary batch size `N` and setting flip to True in order for the LSTM encoder receive input sequence in reversed order. This can help the training.\n"]},{"cell_type":"code","metadata":{"id":"854a1848-fee4-49d4-8e41-698f991b88bb"},"outputs":[],"source":["train_dataloader, valid_dataloader = get_translation_dataloaders(batch_size = 4)#,flip=True)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"5dfe77ff-03f2-46b8-a526-302d1657e8d4"},"source":["You can check the `src` and `trg` tensors:\n"]},{"cell_type":"code","metadata":{"id":"dfb3d383-563f-4181-918e-c1eb6ed96e94"},"outputs":[],"source":["src, trg = next(iter(train_dataloader))\n","src,trg"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"ffee16b6-7860-4ab3-9367-503a54f50d5a"},"source":["You can also get the english and german strings using `index_to_eng` and `index_to_german` functions provided in the .py file:\n"]},{"cell_type":"code","metadata":{"id":"2f0dcb65-8f2a-4816-83e0-75710dab1a12"},"outputs":[],"source":["data_itr = iter(train_dataloader)\n","# moving forward in the dataset to reach sequences of longer length for illustration purpose. (Remember the dataset is sorted on sequence len for optimal padding)\n","for n in range(1000):\n","    german, english= next(data_itr)\n","\n","for n in range(3):\n","    german, english=next(data_itr)\n","    german=german.T\n","    english=english.T\n","    print(\"________________\")\n","    print(\"german\")\n","    for g in german:\n","        print(index_to_german(g))\n","    print(\"________________\")\n","    print(\"english\")\n","    for e in english:\n","        print(index_to_eng(e))\n"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"f43ede0f-34da-4d5d-adc8-5a7a38a14b8f"},"source":["* Note: When working with PyTorch tensors that represent data, it's important to understand the conventions around representing sequences. In most cases, the rows (the first dimension) in a PyTorch tensor represent individual samples, while the columns (the second dimension) represent features or time steps in the case of sequences. When dealing with sequences in PyTorch, it's common to use functions like `pad_sequence` to ensure that all sequences have the same length. Surprisingly, the padding operation is applied along the second dimension (columns), even though sequences are typically represented in the first dimension (rows). This can be confusing at first due to the way batches of sequences are represented. In many sequence-related tasks in PyTorch, especially when working with recurrent models like RNNs, LSTMs, and GRUs, batches of sequences are usually represented with the shape [sequence_length, batch_size, feature_size], where `sequence_length` refers to the length of the longest sequence within the batch(here it is equevalent to `src_len` or `trg_len`). If you check the src tensor above, you can see that the first word of of all sentences are in the first line, the second word of all sentences are in the second line, etc. That is why the first dimension is the length of the sequence.\n","\n","    When you use `pad_sequence`, it adds padding to the sequences in a batch so that they all have the same length, matching the length of the longest sequence. Since sequences are represented in the first dimension, the padding is applied along that dimension. As a result, the output tensor from `pad_sequence` will have the format [sequence_length, batch_size]. (Check the output for `src` and `trg` from the above cell.) This convention is commonly used because models like LSTMs expect the data to be in this format. However, if you're accustomed to working with more traditional tabular data in PyTorch, it can initially cause confusion. It's important to be aware of this convention to avoid potential errors and understand how to properly prepare and format sequence data for your models.\n"]},{"cell_type":"markdown","metadata":{"id":"2d4f7d51-a1b1-4942-b5ea-028858fcc8eb"},"source":["# Training the model\n"]},{"cell_type":"markdown","metadata":{"id":"0c601b72-bec6-40c9-8544-7fc2ab0dcad5"},"source":["### Initializations\n"]},{"cell_type":"markdown","metadata":{"id":"cbd0281c-55fb-4a1f-bd7c-c343ec15cd6d"},"source":["This code sets the random seed for various libraries and modules. This is done to make the results reproducible:\n"]},{"cell_type":"code","metadata":{"id":"b96fa330-7a7c-4029-9391-14280f30e196"},"outputs":[],"source":["SEED = 1234\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"c397d9ce-387b-47bc-9305-4bddd9f7fa61"},"source":["### Training\n","Now, define an instance of the model:\n","\n","- `enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)`: This line creates an instance of the `Encoder` class, which represents the encoder component of the Seq2Seq model. The `Encoder` class takes the input dimension, embedding dimension, hidden dimension, number of layers, and dropout probability as arguments.\n","\n","- `dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)`: This line creates an instance of the `Decoder` class, which represents the decoder component of the Seq2Seq model. The `Decoder` class takes the output dimension, embedding dimension, hidden dimension, number of layers, and dropout probability as arguments.\n","\n","- `model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)`: This line creates an instance of the `Seq2Seq` class, which represents the entire Seq2Seq model. The `Seq2Seq` class takes the encoder, decoder, and device (e.g., CPU or GPU) as arguments. It combines the encoder and decoder to form the complete Seq2Seq architecture.\n"]},{"cell_type":"code","metadata":{"id":"6518e03b-03bb-4d71-ba29-f733c59d639c"},"outputs":[],"source":["INPUT_DIM = len(vocab_transform['de'])\n","OUTPUT_DIM = len(vocab_transform['en'])\n","ENC_EMB_DIM = 128 #256\n","DEC_EMB_DIM = 128 #256\n","HID_DIM = 256 #512\n","N_LAYERS = 1 #2\n","ENC_DROPOUT = 0.3 #0.5\n","DEC_DROPOUT = 0.3 #0.5\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","\n","model = Seq2Seq(enc, dec, device,trg_vocab = vocab_transform['en']).to(device)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"e7037c92-dabe-45e6-88f6-968db1f52e6b"},"source":["`def init_weights(m)`defines a function named `init_weights` that takes a module `m` as input. The purpose of this function is to initialize the weights of the neural network module.\n","\n","The next line `for name, param in m.named_parameters():` starts a loop that iterates over the named parameters of the module `m`. Each parameter is accessed as `param` and its corresponding name is accessed as `name`.\n","\n","`nn.init.uniform_(param.data, -0.08, 0.08)`initializes the parameter's data with values drawn from a uniform distribution between `-0.08` and `0.08`. The `nn.init.uniform_` function is provided by the PyTorch library and is used to initialize the weights of neural network parameters.\n","\n","Finally, `model.apply(init_weights)` applies the `init_weights` function to the `model` instance. This ensures that the weights of all the parameters in the model are initialized using the specified uniform distribution.\n"]},{"cell_type":"code","metadata":{"id":"be483b7b-ac23-4274-8407-8bd9a0545384"},"outputs":[],"source":["def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n","\n","model.apply(init_weights)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"1a635214-1983-4cd6-ae3a-31e0f282a384"},"source":["This code defines a function `count_parameters` that counts the number of trainable parameters in a given model. It then prints the count of trainable parameters in a formatted string.\n"]},{"cell_type":"code","metadata":{"id":"7e525ffc-234b-48ef-a2b4-73a8a32f4ebc"},"outputs":[],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"a94ab08a-7a40-4793-884e-dffd85934ae9"},"source":["The following cell sets up the optimizer and loss function for training the model.\n","\n","1. `optimizer = optim.Adam(model.parameters())`: This line creates an instance of the Adam optimizer and passes the model's parameters (`model.parameters()`) as the parameters to be optimized. The Adam optimizer is a popular optimization algorithm commonly used for training deep neural networks. It adjusts the model's parameters based on the gradients computed during backpropagation to minimize the loss function.\n","\n","2. `PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']`: This line retrieves the index of the `<pad>` token in the target vocabulary.\n","\n","3. `criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)`: This line creates an instance of the CrossEntropyLoss criterion. The CrossEntropyLoss is a commonly used loss function for multi-class classification tasks. In this case, it is used for training the model to predict the next word in the translated sequence. The `ignore_index` parameter is set to `PAD_IDX`, which indicates that the loss should be ignored for any predictions where the target is the padding token. This is useful to exclude padding tokens from contributing to the loss during training.\n"]},{"cell_type":"code","metadata":{"id":"60285ecc-2ca3-4c83-a467-888b9e09b848"},"outputs":[],"source":["optimizer = optim.Adam(model.parameters())\n","\n","PAD_IDX = vocab_transform['en'].get_stoi()['<pad>']\n","\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"c3ac7189-8b0c-434e-be67-078b078c2e84"},"source":["The following helper function provides a convenient way to calculate the elapsed time in minutes and seconds given the start and end times. It will be used to measure the time taken for each epoch during training or any other time-related calculations.\n"]},{"cell_type":"code","metadata":{"id":"2011ce37-c171-45da-b247-7dfe105bcfcc"},"outputs":[],"source":["def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"b2a643e9-9de3-4b6d-b13e-be290c279e90"},"source":["Please be aware that training the model using CPUs can be a time-consuming process. If you don't have access to GPUs, you can jump to \"loading the saved model\" and proceed with loading the pre-trained model using the provided code. The model has been trained for five epochs and saved for your convenience.\n"]},{"cell_type":"markdown","metadata":{"id":"0a45268f-ad64-4d29-9122-a9e12c875f6d"},"source":["Let's start the training epochs:\n"]},{"cell_type":"code","metadata":{"id":"b8a10f99-b180-4499-afde-5c98c047ca7b"},"outputs":[],"source":["torch.cuda.empty_cache()\n","\n","N_EPOCHS = 3 #run the training for at least 5 epochs\n","CLIP = 1\n","\n","best_valid_loss = float('inf')\n","best_train_loss = float('inf')\n","train_losses = []\n","valid_losses = []\n","\n","train_PPLs = []\n","valid_PPLs = []\n","\n","for epoch in range(N_EPOCHS):\n","\n","    start_time = time.time()\n","\n","    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n","    train_ppl = math.exp(train_loss)\n","    valid_loss = evaluate(model, valid_dataloader, criterion)\n","    valid_ppl = math.exp(valid_loss)\n","\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","\n","    if valid_loss < best_valid_loss:\n","\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'RNN-TR-model.pt')\n","\n","    train_losses.append(train_loss)\n","    train_PPLs.append(train_ppl)\n","    valid_losses.append(valid_loss)\n","    valid_PPLs.append(valid_ppl)\n","\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {valid_ppl:7.3f}')\n"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"c4efec51-e441-48b8-bfe6-b543e04f0955"},"source":["Let's visualize the model train and validation losses over the training epochs:\n"]},{"cell_type":"code","metadata":{"id":"765bf1a4-a595-447c-882d-5275b2fac362"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","\n","\n","# Create a list of epoch numbers\n","epochs = [epoch+1 for epoch in range(N_EPOCHS)]\n","\n","# Create the figure and axes\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","ax2 = ax1.twinx()\n","\n","# Plotting the training and validation loss\n","ax1.plot(epochs, train_losses, label='Train Loss', color='blue')\n","ax1.plot(epochs, valid_losses, label='Validation Loss', color='orange')\n","ax1.set_xlabel('Epochs')\n","ax1.set_ylabel('Loss')\n","ax1.set_title('Training and Validation Loss/PPL')\n","\n","# Plotting the training and validation perplexity\n","ax2.plot(epochs, train_PPLs, label='Train PPL', color='green')\n","ax2.plot(epochs, valid_PPLs, label='Validation PPL', color='red')\n","ax2.set_ylabel('Perplexity')\n","\n","# Adjust the y-axis scaling for PPL plot\n","ax2.set_ylim(bottom=min(min(train_PPLs), min(valid_PPLs)) - 10, top=max(max(train_PPLs), max(valid_PPLs)) + 10)\n","\n","# Set the legend\n","lines1, labels1 = ax1.get_legend_handles_labels()\n","lines2, labels2 = ax2.get_legend_handles_labels()\n","lines = lines1 + lines2\n","labels = labels1 + labels2\n","ax1.legend(lines, labels, loc='upper right')\n","\n","\n","# Show the plot\n","plt.show()\n"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"7bc621d2-5886-4924-a2e4-edb62b94e987"},"source":["It can be seen that the loss and perplexity are decreasing as model gets trained. The validation loss starts to stabilize and then grow at Epoch 9, which suggests you do not need to continue training the model to avoid overtraining.\n"]},{"cell_type":"markdown","metadata":{"id":"b7cfa41b-2784-481d-add4-47c1208a0613"},"source":["## Loading the saved model\n","If you want to skip training and load the pre-trained model that is provided, go ahead and uncomment the following cell:\n"]},{"cell_type":"code","metadata":{"id":"9e835eef-0c3f-4e92-85da-7f2dca2ae2fe"},"outputs":[],"source":["!wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/RNN-TR-model.pt'\n","# model.load_state_dict(torch.load('RNN-TR-model.pt',map_location=torch.device('cpu')))"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"bc8a6864-923c-4378-8528-df57cf255afc"},"source":["## Model inference\n","\n","\n","Next, create a generator function that generates translations for input source sentences:\n"]},{"cell_type":"code","metadata":{"id":"9fbce0f0-598e-4193-b147-143a0cd2c23c"},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def generate_translation(model, src_sentence, src_vocab, trg_vocab, max_len=50):\n","    model.eval()  # Set the model to evaluation mode\n","\n","    with torch.no_grad():\n","        src_tensor = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1).to(device)\n","\n","        # Pass the source tensor through the encoder\n","        hidden, cell = model.encoder(src_tensor)\n","\n","        # Create a tensor to store the generated translation\n","        # get_stoi() maps tokens to indices\n","        trg_indexes = [trg_vocab.get_stoi()['<bos>']]  # Start with <bos> token\n","\n","        # Convert the initial token to a PyTorch tensor\n","        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n","\n","        # Move the tensor to the same device as the model\n","        trg_tensor = trg_tensor.to(model.device)\n","\n","\n","        # Generate the translation\n","        for _ in range(max_len):\n","\n","            # Pass the target tensor and the previous hidden and cell states through the decoder\n","            output, hidden, cell = model.decoder(trg_tensor[-1], hidden, cell)\n","\n","            # Get the predicted next token\n","            pred_token = output.argmax(1)[-1].item()\n","\n","            # Append the predicted token to the translation\n","            trg_indexes.append(pred_token)\n","\n","\n","            # If the predicted token is the <eos> token, stop generating\n","            if pred_token == trg_vocab.get_stoi()['<eos>']:\n","                break\n","\n","            # Convert the predicted token to a PyTorch tensor\n","            trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(1)  # Add batch dimension\n","\n","            # Move the tensor to the same device as the model\n","            trg_tensor = trg_tensor.to(model.device)\n","\n","        # Convert the generated tokens to text\n","        # get_itos() maps indices to tokens\n","        trg_tokens = [trg_vocab.get_itos()[i] for i in trg_indexes]\n","\n","        # Remove the <sos> and <eos> from the translation\n","        if trg_tokens[0] == '<bos>':\n","            trg_tokens = trg_tokens[1:]\n","        if trg_tokens[-1] == '<eos>':\n","            trg_tokens = trg_tokens[:-1]\n","\n","        # Return the translation list as a string\n","\n","        translation = \" \".join(trg_tokens)\n","\n","        return translation"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"6557d985-873a-42a0-8efc-12b8145a19fa"},"source":["Now, you can check the model's output for a sample sentence:\n"]},{"cell_type":"code","metadata":{"id":"573b6846-9b7d-4273-be73-bd49758df6ae"},"outputs":[],"source":["# model.load_state_dict(torch.load('RNN-TR-model.pt'))\n","\n","# Actual translation: Asian man sweeping the walkway.\n","src_sentence = 'Ein asiatischer Mann kehrt den Gehweg.'\n","\n","\n","generated_translation = generate_translation(model, src_sentence=src_sentence, src_vocab=vocab_transform['de'], trg_vocab=vocab_transform['en'], max_len=12)\n","#generated_translation = \" \".join(generated_translation_list).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n","print(generated_translation)\n"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"7bc61510-75a7-49d0-a8ec-107b16bd7320"},"source":["Fantastic! You have created a translation model that can generate german-english translations pretty accurate, huh?\n","\n","You can play with the model parameters and hyperparameters to improve the model performance.\n"]},{"cell_type":"markdown","metadata":{"id":"867c2642-2dfa-42c7-ba63-fe9674e28851"},"source":["## BLEU score metric for evaluation\n","While peplexity serves as a general metric to evaluate the performance of language model in predicting the correct next token, BLEU score is helpful in evaluating the quality of the final generated translation.\n","Validating the results using BLEU score is helpful when there is more than a single valid translation for a sentence as you can include many translation versions in the reference list and compare the generated translation with different versions of translations.\n","\n","The BLEU (Bilingual Evaluation Understudy) score is a metric commonly used to evaluate the quality of machine-generated translations by comparing them to one or more reference translations. It measures the similarity between the generated translation and the reference translations based on n-gram matching.\n","\n","The BLEU score is calculated using the following formulas:\n","\n","1. **Precision**:\n","   - Precision measures the proportion of n-grams in the generated translation that appear in the reference translations.\n","   - Precision is calculated for each n-gram order (1 to N) and then combined using a geometric mean.\n","   - The precision for a particular n-gram order is calculated as:\n","   \n","   $$\\text{Precision}_n(t) = \\frac{\\text{CountClip}_n(t)}{\\text{Count}_n(t)}$$\n","   \n","   where:\n","     - $\\text{CountClip}_n(t)$ is the count of n-grams in the generated translation that appear in any reference translation, clipped by the maximum count of that n-gram in any single reference translation.\n","     - $\\text{Count}_n(t)$ is the count of n-grams in the generated translation.\n","\n","2. . **Brevity penalty**:\n","   - The brevity penalty accounts for the fact that shorter translations tend to have higher precision scores.\n","   - It encourages translations that are closer in length to the reference translations.\n","   - The brevity penalty is calculated as:\n","   \n","  $$\\text{BP} = \\begin{cases} 1 & \\text{if } c > r \\\\\\\\\\\\\\\\\\\\\\\\\\\\ e^{(1 - \\frac{r}{c})} & \\text{if } c \\leq r \\end{cases}$$\n","   \n","   where:\n","     - $c$ is the total length of the generated translation.\n","     - $r$ is the total length of the reference translations.\n","\n","3. **BLEU score**:\n","   - The BLEU score is the geometric mean of the precisions, weighted by the brevity penalty.\n","   - It is calculated as:\n","   \n","   $$\\text{BLEU} = \\text{BP} \\cdot \\exp(\\sum_{n=1}^{N}w_n \\log(\\text{Precision}_n(t)))$$\n","   \n","   where:\n","     - $N$ is the maximum n-gram order.\n","     - $w_n$ is the weight assigned to the precision at n-gram order $n$, commonly set as $\\frac{1}{N}$ for equal weights.\n"]},{"cell_type":"code","metadata":{"id":"c40af847-360d-4e5f-8754-ebaf5f255660"},"outputs":[],"source":["def calculate_bleu_score(generated_translation, reference_translations):\n","    # Convert the generated translations and reference translations into the expected format for sentence_bleu\n","    references = [reference.split() for reference in reference_translations]\n","    hypothesis = generated_translation.split()\n","\n","    # Calculate the BLEU score\n","    bleu_score = sentence_bleu(references, hypothesis)\n","\n","    return bleu_score"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"14713d5f-93cf-4f6d-948f-cfce61d39260"},"source":["Let's calculate the BLEU score for a sample sentence:\n"]},{"cell_type":"code","metadata":{"id":"49469276-6fca-43b1-a746-dc21992cf353"},"outputs":[],"source":["reference_translations = [\n","    \"Asian man sweeping the walkway .\",\n","    \"An asian man sweeping the walkway .\",\n","    \"An Asian man sweeps the sidewalk .\",\n","    \"An Asian man is sweeping the sidewalk .\",\n","    \"An asian man is sweeping the walkway .\",\n","    \"Asian man sweeping the sidewalk .\"\n","]\n","\n","bleu_score = calculate_bleu_score(generated_translation, reference_translations)\n","print(\"BLEU Score:\", bleu_score)"],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"461f4054-7c2d-47b8-a2df-fc6658189752"},"source":["# Exercises\n"]},{"cell_type":"markdown","metadata":{"id":"4df6a8b8-8fca-4d7b-8313-437eced45356"},"source":["### Exercise 1 - Translate a German sentence to English.\n"]},{"cell_type":"code","metadata":{"id":"c4f45b52-2c52-4d57-843c-dfef82ce9a3e"},"outputs":[],"source":["# Define the German text to be translated\n","german_text = \"Menschen gehen auf der Straße\"\n","\n","..."],"execution_count":null},{"cell_type":"markdown","metadata":{"id":"11afc146-73ab-4bf0-8a6b-61ac8ad2d8a9"},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","german_text = \"Menschen gehen auf der Straße\"\n","\n","# The function should be defined to accept the text, the model, source and target vocabularies, and the device as parameters.\n","english_translation = generate_translation(\n","    model,\n","    src_sentence=german_text,\n","    src_vocab=vocab_transform['de'],\n","    trg_vocab=vocab_transform['en'],\n","    max_len=50\n",")\n","\n","# Display the original and translated text\n","print(f\"Original German text: {german_text}\")\n","print(f\"Translated English text: {english_translation}\")\n","```\n","\n","</details>\n"]},{"cell_type":"markdown","metadata":{"id":"26977ec4-3cec-4b6b-912c-a9ff17834acc"},"source":["## Authors\n"]},{"cell_type":"markdown","metadata":{"id":"caa72846-a65e-4dec-a900-d9d5f5757ce1"},"source":["[Fateme Akbari](https://www.linkedin.com/in/fatemeakbari/) is a PhD candidate in Information Systems at McMaster University with demonstrated research experience in Machine Learning and NLP.\n"]},{"cell_type":"markdown","metadata":{"id":"1ab69890-4ef7-40ef-906b-4545e9bedf41"},"source":["```{## Change log}\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"ca4020ae-8810-4653-bf78-7da23cff0a8f"},"source":["```{|Date (YYYY-MM-DD)|Version|Changed By|Change Description||-|-|-|-||2020-07-17|0.1|Sam|Create Lab Template|}\n","```\n"]},{"cell_type":"markdown","metadata":{"id":"0cec3667-d60e-4852-b1e8-b0c3ab63b040"},"source":["© Copyright IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"prev_pub_hash":"4f4e0823d6dfdba7b337e23792dc99caa0f2879a67d48086b8c7db5707249d8e","colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}